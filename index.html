<!DOCTYPE html>
<html>
<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>HoRoPose: Real-time Holistic Robot Pose Estimation with Unknown States</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Courier Prime' rel='stylesheet'>
    <style> 
        span {
          font-family: Courier;
        }
        p {
            font-size: 17px;
        }
    </style>
</head>



<body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                <b>
                    <span style="color: #3498DB; font-family: inherit;">Ho</span><span style="color: #FF5733; font-family: inherit;">Ro</span><span style="color: #9B59B6; font-family: inherit;">Pose</span>: Real-time <span style="color: #3498DB; font-family: inherit;">Ho</span>listic <span style="color: #FF5733; font-family: inherit;">Ro</span>bot <span style="color: #9B59B6; font-family: inherit;">Pose</span> Estimation with Unknown States
                </b>
            
            
        </div>
        <br>
        <div class="row">
            <p style="text-align:center; font-size: 22px"> ECCV 2024 </p>
        </div>
        <br>
        <div class="row">
            <div class="col-md-12 text-center" style="font-size: 24px">
                <ul class="list-inline" style="font-size: 20px; color: #4B527E">
                    <li>
                        <a href="https://oliverbansk.github.io/", target="_blank", style="font-size: 20px; color: #4B527E">Shikun Ban</a>
                    </li>
                    <li>
                        <h style="font-size: 20px; color: #4B527E">Juling Fan</h>
                    </li>
                    <li>
                        <a href="https://shirleymaxx.github.io/", target="_blank", style="font-size: 20px; color: #4B527E">Xiaoxuan Ma</a>
                    </li>
                    <li>
                        <a href="https://wentao.live/", target="_blank", style="font-size: 20px; color: #4B527E">Wentao Zhu</a>
                    </li>
                    <li>
                        <a href="http://www.pami.sjtu.edu.cn/yuqiao", target="_blank", style="font-size: 20px; color: #4B527E">Yu Qiao</a>
                    </li>
                    <li>
                        <a href="https://cfcs.pku.edu.cn/english/people/faculty/yizhouwang/index.htm", target="_blank", style="font-size: 20px; color: #4B527E">Yizhou Wang</a>
                    </li>
                </ul>
                <h style="font-size: 18px; color: black">Peking University, Shanghai Jiao Tong University</h>
            </div>
        </div>

        <br>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="nav nav-pills nav-justified" style="margin-top:10px;">
                    <a href="https://arxiv.org/abs/2402.05655" target="_blank" ,="" style="color: #3498DB;">
                            <strong><font size="+2">[Paper]</font></strong>
                    </a>
                    &nbsp;&nbsp;&nbsp;
                    <a href="https://github.com/Oliverbansk/Hollistic-Robot-Pose-Estimation", target="_blank" ,="" style="color: #FF5733;">
                            <strong><font size="+2">[Code]</font></strong>
                    </a>
                    &nbsp;&nbsp;&nbsp;
                    <a href="https://www.youtube.com/watch?v=9NsLJvp1IPE" target="_blank" ,="" style="color: #9B59B6;">
                            <strong><font size="+2">[Video]</font></strong>
                    </a>
                    &nbsp;&nbsp;&nbsp;
                    <a href="https://x.com/OliverBan2/status/1842139593476608413" target="_blank" ,="" style="color: #24c0a6;">
                            <strong><font size="+2">[Thread]</font></strong>
                    </a>
                </ul>
            </div>
        </div>

        <div class="row ">
            <br>
            <div class="col-md-8 col-md-offset-1">
                <img src="./roc/fig1_v13.png" width="120%" style="border-style: none">
                
            </div>
            <!-- <div class="col-md-8 col-md-offset-2">
                <p class="text-justify" style="font-size:15px">
                    The majority of previous works assume known robot joint states and focus solely on estimating the camera-to-robot pose. 
                    In contrast, the <strong>holistic robot pose estimation problem (our setting)</strong> 
                    requires estimating both joint states and the camera-to-robot pose, 
                    given only an RGB image without known joint states. 
                </p>
                <p class="text-justify" style="font-size:15px">
                    For holistic robot pose estimation, 
                    Previous work (RoboPose) uses costly test-time optimization (Render-and-Compare) to iteratively refine the predictions. 
                    In contrast, <strong>our feed-forward method achieves state-of-the-art accuracy with a 12-time speed boost</strong>.
                </p>   
            </div> -->
        </div>


        <div class="row ">
            <br>
            <div class="col-md-8 col-md-offset-2 ">
                <h2>
                    <b>Abstract</b>
                </h2>
                <p class="text-justify">
                    Estimating robot pose from RGB images is a crucial problem in computer vision and robotics. 
                    While previous methods have achieved promising performance, 
                    most of them presume full knowledge of robot internal states, <em>e.g.</em> ground-truth robot joint angles. 
                    However, this assumption is not always valid in practical situations. 
                    In real-world applications such as multi-robot collaboration or human-robot interaction, 
                    the robot joint states might not be shared or could be unreliable. 
                    On the other hand, existing approaches that estimate robot pose without joint state priors suffer from heavy computation burdens 
                    and thus cannot support real-time applications. 
                </p>
                <p class="text-justify">
                    This work introduces an efficient framework for real-time robot pose estimation from RGB images without requiring known robot states. 
                    Our method estimates camera-to-robot rotation, robot state parameters, keypoint locations, and root depth, 
                    employing a neural network module for each task to facilitate learning and sim-to-real transfer. 
                    Notably, it achieves inference in a single feedforward pass without iterative optimization. 
                    Our approach offers a <strong>12-time speed increase</strong> with <strong>state-of-the-art accuracy</strong>, 
                    enabling <strong>real-time holistic robot pose estimation</strong> for the first time.
                </p>
            </div>
        </div>

        <div class="row ">
            <br>
            <div class="col-md-8 col-md-offset-2 ">
                <h2>
                    <b>Framework</b>
                </h2>
                <img src="./roc/framework_v9.png" width="100%" style="border-style: none">

                <p class="text-justify"><br>
                    We factorize the holistic robot pose estimation task into several sub-tasks, including the estimation of camera-to-robot rotation, robot joint states, root depth, and root-relative keypoint locations. We then design corresponding neural network modules for each sub-task.
                </p>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <br>
                <h2>
                    <b>Results</b>
                </h2>
                <h3 style="font-weight: 500;font-size: 20px">Qualitative Comparison on DREAM dataset</h3>
                <!-- <h4><b>Case 1</b></h4> -->
                <!-- <p><br><b>Panda Azure:</b></p> -->
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <b style="font-size: 15px;word-spacing: 198px;">Input RoboPose Ours</b>
                
                <!-- <img src="./roc/azure_36.gif" width="100%" style="border-style: none"> -->
                <!-- <p><br><b>Panda Kinect:</b></p> -->
                
                <!-- <img src="./roc/kinect_225_240.gif" width="100%" style="border-style: none"> -->
                <!-- <img src="./roc/video_rs_1.mp4" width="100%" style="border-style: none"> -->
                <video width="750" height="200" style="border-style: none" autoplay muted loop>
                    <source src="./roc/video_rs_1.mp4" type="video/mp4">
                </video>
                <!-- <p><br><b>Panda Realsense:</b></p> -->
                
                <!-- <img src="./roc/realsense.gif" width="100%" style="border-style: none"> -->
                <!-- <p><br><b>Panda Orb:</b></p> -->
                
                <!-- <img src="./roc/orb_1049.gif" width="100%" style="border-style: none"> -->
                <video width="750" height="200" style="border-style: none" autoplay muted loop>
                    <source src="./roc/video_orb_1.mp4" type="video/mp4">
                </video>
                <!-- <img src="./roc/azure_0-21.gif" width="100%" style="border-style: none"> -->
                
                <p><br>
                    Qualitative comparison presented above illustrates our approach's ability to produce more stable and accurate estimates in multiple videos from the real-world Panda Datasets.
                </p>

                <br>
                <!-- <h3 style="font-weight: 500;font-size: 20px">Qualitative Comparison with RoboPose on Images from 3 Different Robots</h3> -->
                <!-- <p><br><b>Case 1:</b></p> -->
                <img src="./roc/qualitative_results_synthetic.png" width="100%" style="border-style: none">
                
                <p><br>
                    Qualitative comparison on images of different robots is presented above. It illustrates that our approach's ability to produce high-quality estimates across different robot morphologies. Particularly, our approach demonstrates superior performance in highly challenging cases, including self-occlusions, truncations, and extreme lighting conditions.
                </p>
                
                <br>
                <h3 style="font-weight: 500;font-size: 20px">Qualitative Comparison on In-the-wild Images</h3>
                <img src="./roc/qualitative_results_real.png" width="100%" style="border-style: none">

                <!-- <br><br>
                    <ul>
                      <li><p>[Level-1] Player <i style="color: #8684ff;">Purple</i> passes to Player <i style="color: #6296f3;">Blue</i>.</p></li>
                      <li><p>[Level-2] Accordingly, Player <i style="color: #e1a95d;">Brown</i> generates the stealing action and causes a defense gap between Player <i style="color: #6296f3;">Blue</i> and Player <i style="color: #6db9c0;">Cyan</i>.</p></li>
                      <li><p>[Level-3] Therefore, Player <i style="color: #ff8902;">Orange</i> turns around and steps into the middle of player <i style="color: #6296f3;">Blue</i> and Player <i style="color: #6db9c0;">Cyan</i> to fill the gap, which is closer to expert demonstration.</p></li>
                    </ul> -->
        
                <!-- <p><br><b>Case 2:</b></p>
                <img src="./roc/ch_2.gif" width="100%" style="border-style: none">

                <br><br>
                <ul>
                    <li><p>[Level-1] Player <i style="color:#8684ff">Purple</i> makes a reception and a passing move.</p></li>
                    <li><p>[Level-2] Thus, as a defender, Player <i style="color:#e1a95d">Brown</i> generates the action to turn to Player <i style="color:#8684ff">Purple</i>. His teammate, Player <i style="color:#ff8902">Orange</i>, is also getting close to Player <i style="color:#8684ff">Purple</i> to defend.</p></li>
                    <li><p>[Level-3] In light of this, Player <i style="color:#ff8902">Orange</i> chooses a different action, turning to Player <i style="color:#6db9c0">Cyan</i>. The level-3 actions are also closer to ground truth.</p></li>
                </ul> -->

                <p><br>
                    Qualitative results for in-the-wild laboratory images are presented above. No markers were used in the process. For instance, in the image located in the upper row, our method demonstrates a more accurate estimation of the pose of the robot's base.
                </p>

            </div>
        </div>

<br>
<div class="row">
    <div class="col-md-8 col-md-offset-2">
        <h2>
            <b>Citation</b>
        </h2>
        <div class="form-group col-md-10 col-md-offset-1">
            <div class="CodeMirror cm-s-default CodeMirror-wrap">
                <div style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 4px; left: 4px;">
                    <textarea autocorrect="off" autocapitalize="off" spellcheck="false" style="position: absolute; padding: 0px; width: 1000px; height: 1em; outline: none;" tabindex="0">
                    </textarea>
                </div>
                <div class="CodeMirror-vscrollbar" cm-not-content="true" style="min-width: 18px;">
                    <div style="min-width: 1px; height: 0px;"></div>
                </div>
                <div class="CodeMirror-hscrollbar" cm-not-content="true" style="min-height: 18px;">
                    <div style="height: 100%; min-height: 1px; width: 0px;"></div>
                </div>
                <div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div>
                <div class="CodeMirror-gutter-filler" cm-not-content="true"></div>
                <div class="CodeMirror-scroll" tabindex="-1">
                    <div class="CodeMirror-sizer" style="margin-left: 0px; margin-bottom: 0px; border-right-width: 30px; min-height: 214px; padding-right: 0px; padding-bottom: 0px;">
                        <div style="position: relative; top: 0px;">
                            <div class="CodeMirror-lines">
                                <div style="position: relative; outline: none;">
                                    <div class="CodeMirror-measure">AخA</div>
                                    <div class="CodeMirror-measure"></div>
                                    <div style="position: relative; z-index: 1;"></div>
                                    <div class="CodeMirror-cursors">
                                        <div class="CodeMirror-cursor" style="left: 4px; top: 0px; height: 20.5625px;">&nbsp;</div>
                                    </div>
                                    <div class="CodeMirror-code" style="">
                                        <pre class=" CodeMirror-line "><span style="padding-right: 0.1px;">@inproceedings{holisticrobotpose,</span></pre>
                                        <pre class=" CodeMirror-line "><span style="padding-right: 0.1px;"> &nbsp;  author={Ban, Shikun and Fan, Juling and Ma, Xiaoxuan and Zhu, Wentao and Qiao, Yu and Wang, Yizhou},</span></pre>
                                        <pre class=" CodeMirror-line "><span style="padding-right: 0.1px;"> &nbsp;  title={Real-time Holistic Robot Pose Estimation with Unknown States},</span></pre>
                                        <pre class=" CodeMirror-line "><span style="padding-right: 0.1px;"> &nbsp;  booktitle = {European Conference on Computer Vision (ECCV)},</span></pre>
                                        <pre class=" CodeMirror-line "><span style="padding-right: 0.1px;"> &nbsp;  year = {2024}</span></pre>
                                        <pre class=" CodeMirror-line "><span style="padding-right: 0.1px;">}</span></pre>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div style="position: absolute; height: 30px; width: 1px; top: 214px;"></div>
                    <div class="CodeMirror-gutters" style="display: none; height: 244px;"></div>
                </div>
            </div>
        </div>
    </div>
</div>

<br><br><br><br><br><br><br><br>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p style="text-align:center;font-size:small;">
                    Template courtesy of <a href="https://jonbarron.info/" ,="" target="_blank">Jon Barron</a>.
                  </p>
            </div>
        </div>
    </div>
</body>


</html>
